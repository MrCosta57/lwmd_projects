{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Massive Data\n",
    "### Assignment 2 - Studying Sparse-Dense Retrieval\n",
    "#### Giovanni Costa - 880892"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents:\n",
    "- [Sparse representation](#s_repr)\n",
    "- [Dense representation](#d_repr)\n",
    "- [Top k retrieval](#exact_retr)\n",
    "- [Top k\\' retrieval (approximate case)](#approx_retr)\n",
    "- [Evaluations](#eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import PorterStemmer\n",
    "from matplotlib import pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "data_dir=\"datasets/\"\n",
    "dataset1_name_dir=\"trec-covid/\"\n",
    "dataset2_name_dir=\"scifact/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df1=pd.read_json(data_dir+dataset1_name_dir+\"corpus.jsonl\", lines=True)\n",
    "queries_df1=pd.read_json(data_dir+dataset1_name_dir+\"queries.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df2=pd.read_json(data_dir+dataset2_name_dir+\"corpus.jsonl\", lines=True)\n",
    "queries_df2=pd.read_json(data_dir+dataset2_name_dir+\"queries.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df2.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='s_repr'></a>\n",
    "### Sparse representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sk-learn's \"TfidfVectorizer\" and \"CountVectorizer\" extension to provide the stemming feature\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    stemmer = PorterStemmer()\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (StemmedTfidfVectorizer.stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "class StemmedTfidfCountVectorizer(CountVectorizer):\n",
    "    stemmer = PorterStemmer()\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (StemmedTfidfCountVectorizer.stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "    \n",
    "def compute_sparse_repr(vocab: np.array, corpus: pd.DataFrame, queries: pd.DataFrame):\n",
    "    #Extract only the word and the numbers, made a lowercase transformation and usage of custom vocabulary to make representations independent\n",
    "    doc_tfidf=StemmedTfidfVectorizer(lowercase=True, vocabulary=vocab, stop_words=None, token_pattern=r'\\w+')\n",
    "    q_counter=StemmedTfidfCountVectorizer(lowercase=True, vocabulary=vocab, stop_words=None, token_pattern=r'\\w+')\n",
    "\n",
    "    #Computation of the sparse embedding\n",
    "    sparse_doc=doc_tfidf.fit_transform(corpus[\"text\"])\n",
    "    sparse_q=q_counter.fit_transform(queries[\"text\"])\n",
    "\n",
    "    return sparse_doc, sparse_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem the vocabulary and drop the duplicates\n",
    "stemmer=PorterStemmer()\n",
    "vocab=np.unique([stemmer.stem(w) for w in np.char.lower(words.words())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_doc, sparse_q=compute_sparse_repr(vocab, corpus_df1, queries_df1)\n",
    "\n",
    "#Here it's basically computed sparse_score=<q_sparse, d_sparse>\n",
    "sparse_score_df=pd.DataFrame(np.dot(sparse_q, sparse_doc.transpose()).toarray(), index=queries_df1[\"_id\"], columns=corpus_df1[\"_id\"])\n",
    "sparse_score_df.to_parquet(\"sparse_score_df_\"+dataset1_name_dir.split(\"/\")[0]+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_doc, sparse_q=compute_sparse_repr(vocab, corpus_df2, queries_df2)\n",
    "\n",
    "#Here it's basically computed sparse_score=<q_sparse, d_sparse>\n",
    "sparse_score_df=pd.DataFrame(np.dot(sparse_q, sparse_doc.transpose()).toarray(), index=queries_df2[\"_id\"], columns=corpus_df2[\"_id\"])\n",
    "sparse_score_df.to_parquet(\"sparse_score_df_\"+dataset2_name_dir.split(\"/\")[0]+\".parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='d_repr'></a>\n",
    "### Dense representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dense_repr(corpus: pd.DataFrame, queries: pd.DataFrame):\n",
    "    transformers = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    ##Computation of the dense embedding\n",
    "    dense_c=transformers.encode(corpus[\"text\"], convert_to_numpy = True)\n",
    "    dense_q=transformers.encode(queries[\"text\"], convert_to_numpy = True)\n",
    "    \n",
    "    return dense_c, dense_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_c, dense_q=compute_dense_repr(corpus_df1, queries_df1)\n",
    "\n",
    "#Here it's basically computed dense_score=<q_dense, d_dense>\n",
    "dense_score_df=pd.DataFrame(np.dot(dense_q, dense_c.transpose()), index=queries_df1[\"_id\"], columns=corpus_df1[\"_id\"])\n",
    "dense_score_df.to_parquet(\"dense_score_df_\"+dataset1_name_dir.split(\"/\")[0]+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_c, dense_q=compute_dense_repr(corpus_df2, queries_df2)\n",
    "\n",
    "#Here it's basically computed dense_score=<q_dense, d_dense>\n",
    "dense_score_df=pd.DataFrame(np.dot(dense_q, dense_c.transpose()), index=queries_df2[\"_id\"], columns=corpus_df2[\"_id\"])\n",
    "dense_score_df.to_parquet(\"dense_score_df_\"+dataset2_name_dir.split(\"/\")[0]+\".parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exact_retr'></a>\n",
    "## Top k retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_score_df1=pd.read_parquet(\"sparse_score_df_\"+dataset1_name_dir.split(\"/\")[0]+\".parquet\")\n",
    "dense_score_df1=pd.read_parquet(\"dense_score_df_\"+dataset1_name_dir.split(\"/\")[0]+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_score_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_score_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_score_df2=pd.read_parquet(\"sparse_score_df_\"+dataset2_name_dir.split(\"/\")[0]+\".parquet\")\n",
    "dense_score_df2=pd.read_parquet(\"dense_score_df_\"+dataset2_name_dir.split(\"/\")[0]+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_score_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_score_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_retrieval(sparse_score_df: pd.DataFrame, dense_score_df: pd.DataFrame, k: int):\n",
    "    #Sum the two scores\n",
    "    total_score_df=sparse_score_df+dense_score_df\n",
    "\n",
    "    #Get the exact top-k documents indexes\n",
    "    idx_exact_top_k=np.argsort(total_score_df)[:, :-k-1:-1]\n",
    "    #Get the exact top-k documents ids\n",
    "    top_k_exact_docs=np.array(total_score_df.columns[idx_exact_top_k.reshape(-1)]).reshape(-1, k)\n",
    "\n",
    "    return total_score_df, top_k_exact_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "total_score_df1, top_k_exact_docs_df1=compute_exact_retrieval(sparse_score_df1, dense_score_df1, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score_df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_exact_docs_df1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "total_score_df2, top_k_exact_docs_df2=compute_exact_retrieval(sparse_score_df2, dense_score_df2, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score_df2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_exact_docs_df2[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='approx_retr'></a>\n",
    "## Top k' retrieval (approximate case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_approx_retrieval(sparse_score_df: pd.DataFrame, dense_score_df: pd.DataFrame, total_score_df: pd.DataFrame, k_prime: int):\n",
    "    #Get the indexes of top-k' docs for the sparse representations\n",
    "    idx_sparse_scores=np.argsort(sparse_score_df)[:, :-k_prime-1:-1]\n",
    "    #Get the indexes of top-k' docs for the dense representations\n",
    "    idx_dense_scores=np.argsort(dense_score_df)[:, :-k_prime-1:-1]\n",
    "\n",
    "    #Merging of the two vectors of indexes\n",
    "    concat_idx=np.concatenate((idx_sparse_scores, idx_dense_scores), axis=1)\n",
    "    #Drop duplicates in every row\n",
    "    union_idx=[np.unique(x) for x in concat_idx]\n",
    "\n",
    "    #Get the indexes of the top-k documents given the merged approximate sets:\n",
    "    #for every query, take the top-k indexes using the specific indexes of the merged set, based on total_score computed previous\n",
    "    idx_approx_top_k=np.asarray([ union_idx[i][np.argsort(total_score_df.iloc[i, union_idx[i]])[:-k-1:-1].values]\n",
    "                                  for i in range(len(union_idx)) ])\n",
    "    \n",
    "    #Get the doc ids from the retrieved indexes\n",
    "    top_k_approx_docs=np.array(total_score_df.columns[idx_approx_top_k.reshape(-1)]).reshape(-1, k)\n",
    "\n",
    "    return top_k_approx_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_approx_docs_df1=compute_approx_retrieval(sparse_score_df1, dense_score_df1, total_score_df1, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_approx_docs_df1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_approx_docs_df2=compute_approx_retrieval(sparse_score_df2, dense_score_df2, total_score_df2, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_approx_docs_df2[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval'></a>\n",
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_fun(sparse_score_df: pd.DataFrame, dense_score_df: pd.DataFrame, corpus_len: int, k_list: list, step:int=1, epsilon:float=0.01):\n",
    "    mean_recalls_list=[[], [], []]\n",
    "    top_k_exact_approx_lists=[[], [], []]\n",
    "    max_k_prime_list=[]\n",
    "    i=0\n",
    "    for k in k_list:\n",
    "        total_score_df, top_k_exact_docs=compute_exact_retrieval(sparse_score_df, dense_score_df, k)\n",
    "        for k_prime in range(k, corpus_len+1, step):\n",
    "            top_k_approx_docs=compute_approx_retrieval(sparse_score_df, dense_score_df, total_score_df, k_prime)\n",
    "            recalls=[len(np.intersect1d(top_k_exact_docs[i], top_k_approx_docs[i], assume_unique=True))/k for i in range(len(top_k_exact_docs))]\n",
    "            mean=np.mean(recalls)\n",
    "            mean_recalls_list[i].append(mean)\n",
    "\n",
    "            if mean>=1-epsilon or k==corpus_len:\n",
    "                print(k_prime)\n",
    "                max_k_prime_list.append(k_prime)\n",
    "                break\n",
    "        i+=1\n",
    "    \n",
    "    return mean_recalls_list, top_k_exact_approx_lists, max_k_prime_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_plot(k_list, mean_recalls_list, top_k_exact_approx_lists, max_k_prime_list, step=1):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.ylabel('Recall scores')\n",
    "    plt.xlabel('K\\' values')\n",
    "    plt.ylim(np.min(np.concatenate(mean_recalls_list)), 1)\n",
    "    plt.hlines(np.max(np.concatenate(mean_recalls_list)), np.min(k_list), np.max(max_k_prime_list), linewidth=2, linestyles=\"dashed\", colors=\"grey\")\n",
    "\n",
    "    for i in range(len(k_list)):    \n",
    "        plt.plot(range(k_list[i], max_k_prime_list[i]+1, step), mean_recalls_list[i], linewidth=2, legend=str(i))\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list=[5, 50, 100]\n",
    "mean_recalls_list_df1, top_k_exact_approx_lists_df1, max_k_prime_list_df1=evaluation_fun(sparse_score_df1, dense_score_df1, len(corpus_df1), k_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(k_list, mean_recalls_list_df1, top_k_exact_approx_lists_df1, max_k_prime_list_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_recalls_list_df2, top_k_exact_approx_lists_df2, max_k_prime_list_df2=evaluation_fun(sparse_score_df2, dense_score_df2, len(corpus_df2), k_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_plot(k_list, mean_recalls_list_df2, top_k_exact_approx_lists_df2, max_k_prime_list_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k_list)):\n",
    "    print(\"K: \", k_list[i])\n",
    "    idx=np.argmax(mean_recalls_list_df1[i])\n",
    "    print(\"Exit threshold: \", 1-0.01)\n",
    "    print(\"Highest recall w.r.t. exact solution: \", mean_recalls_list_df1[i][idx], \",\")\n",
    "    print(\"obtained with k\\': \", max_k_prime_list_df1[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k_list)):\n",
    "    print(\"K: \", k_list[i])\n",
    "    idx=np.argmax(mean_recalls_list_df2[i])\n",
    "    print(\"Exit threshold: \", 1-0.01)\n",
    "    print(\"Highest recall w.r.t. exact solution: \", mean_recalls_list_df2[i][idx], \",\")\n",
    "    print(\"obtained with k\\': \", max_k_prime_list_df2[i])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lwmd_ass2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
