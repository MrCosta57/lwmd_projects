{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Massive Data\n",
    "<p>\n",
    "Assignment 3 - Similarity search for document pairs<br>\n",
    "Giovanni Costa - 880892\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents:\n",
    "- [Sparse representation](#s_repr)\n",
    "- [Dense representation](#d_repr)\n",
    "- [Top k retrieval](#exact_retr)\n",
    "- [Top k\\' retrieval (approximate case)](#approx_retr)\n",
    "- [Evaluations](#eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.sparse import load_npz, save_npz\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import PorterStemmer\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.random_projection import SparseRandomProjection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document sparse representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"datasets/nfcorpus/corpus.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs=pd.read_json(dataset, lines=True)\n",
    "df_docs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sk-learn's \"TfidfVectorizer\" extension to provide the stemming feature\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    stemmer = PorterStemmer()\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (StemmedTfidfVectorizer.stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "    \n",
    "def compute_sparse_repr(corpus: pd.DataFrame):\n",
    "    #Extract only the word and the numbers, made a lowercase transformation and usage of custom vocabulary to make representations independent\n",
    "    doc_tfidf=StemmedTfidfVectorizer(lowercase=True, stop_words=None, token_pattern=r'\\w+', binary=True)\n",
    "\n",
    "    #Computation of the sparse embedding\n",
    "    sparse_doc=doc_tfidf.fit_transform(corpus[\"text\"])\n",
    "    vocab=doc_tfidf.vocabulary_\n",
    "    \n",
    "    return sparse_doc, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_repr, vocab=compute_sparse_repr(df_docs)\n",
    "save_npz(\"sparse_repr_nfcorpus.npz\", sparse_repr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Implementation \n",
    "### Exact similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_repr=load_npz(\"sparse_repr_nfcorpus.npz\")\n",
    "print(sparse_repr.shape)\n",
    "print(\"Density ratio:\", sparse_repr.count_nonzero()/(sparse_repr.shape[0]*sparse_repr.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "cosine_scores=cosine_similarity(sparse_repr, dense_output=False)\n",
    "np.fill_diagonal(cosine_scores, -1)\n",
    "num_of_pairs=(cosine_scores>=threshold).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_of_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "#tmp=pd.DataFrame(cosine_scores, columns=df_docs[\"_id\"], index=df_docs[\"_id\"])\n",
    "x, y=np.where(tmp>=0.5)\n",
    "for i, j in zip(tmp.index[x], tmp.columns[y]):\n",
    "    print(i, j) \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate similarity search\n",
    "(using Sparse Random Projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_proj=SparseRandomProjection(eps=epsilon, random_state=32)\n",
    "sr_proj.fit(sparse_repr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sr_proj.n_components_)\n",
    "print(sr_proj.density_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_repr_approx_srp=sr_proj.transform(sparse_repr)\n",
    "print(sparse_repr_approx_srp.shape)\n",
    "print(\"Density ratio:\", sparse_repr_approx_srp.count_nonzero()/(sparse_repr_approx_srp.shape[0]*sparse_repr_approx_srp.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "sparse_repr_approx_srp=sr_proj.transform(sparse_repr)\n",
    "cosine_scores_approx_srp=cosine_similarity(sparse_repr_approx_srp, dense_output=False)\n",
    "np.fill_diagonal(cosine_scores_approx_srp, -1)\n",
    "num_of_pairs_approx_srp=(cosine_scores_approx_srp>=threshold).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_pairs_approx_srp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'C:\\ProgramData\\mambaforge\\envs\\ML-base\\python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'C:\\ProgramData\\mambaforge\\envs\\ML-base\\Scripts\\ipython.exe'\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_repr=load_npz(\"sparse_repr_nfcorpus.npz\")\n",
    "print(sparse_repr.shape)\n",
    "\n",
    "sorted_pairs = sorted(vocab.items(), key=lambda x: x[1])\n",
    "vocab_terms = [pair[0] for pair in sorted_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(np.dot(a,b))\n",
    "index = np.argmax(cumsum < threshold)\n",
    "result = cumsum[index] #b(d) value\n",
    "\n",
    "\n",
    "def my_map(id, doc_repr, sorted_idx, ):\n",
    "    for idx in sorted_idx:\n",
    "        if doc_repr[idx]>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" sparse_df=pd.DataFrame.sparse.from_spmatrix(sparse_repr)\n",
    "sdf = ps.from_pandas(sparse_df)\n",
    "sdf.max(axis=0)\n",
    "\n",
    "sdf.apply() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.applymap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def evaluation_fun(total_score_df: pd.DataFrame, \n",
    "                   idx_sparse: np.array, idx_dense: np.array, idx_total: np.array,                   \n",
    "                   corpus_len: int, k_list: list, step:int=1, epsilon:float=0.01):\n",
    "    mean_recalls_list=[list() for i in range(len(k_list))]\n",
    "    top_k_exact_approx_lists=[list() for i in range(len(k_list))]\n",
    "    max_k_prime_list=[]\n",
    "    i=0\n",
    "    for k in k_list:\n",
    "        top_k_exact_docs=compute_exact_retrieval(total_score_df, idx_total, k)\n",
    "        for k_prime in range(k, corpus_len+1, step):\n",
    "            top_k_approx_docs=compute_approx_retrieval(idx_sparse, idx_dense, total_score_df, k, k_prime)\n",
    "            recalls=[len(np.intersect1d(top_k_exact_docs[i], top_k_approx_docs[i], assume_unique=True))/k for i in range(len(top_k_exact_docs))]\n",
    "            mean=np.mean(recalls)\n",
    "            mean_recalls_list[i].append(mean)\n",
    "\n",
    "            if mean>=1-epsilon or k==corpus_len:\n",
    "                print(k_prime)\n",
    "                max_k_prime_list.append(k_prime)\n",
    "                break\n",
    "        i+=1\n",
    "    \n",
    "    return mean_recalls_list, top_k_exact_approx_lists, max_k_prime_list\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def print_plot(k_list, mean_recalls_list, max_k_prime_list, step=1):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.ylabel('Recall scores')\n",
    "    plt.xlabel('K\\' values')\n",
    "    plt.ylim(np.min(np.concatenate(mean_recalls_list)), 1)\n",
    "    plt.hlines(np.max(np.concatenate(mean_recalls_list)), np.min(k_list), np.max(max_k_prime_list), linewidth=2, linestyles=\"dashed\", colors=\"grey\")\n",
    "\n",
    "    for i in range(len(k_list)):    \n",
    "        plt.plot(range(k_list[i], max_k_prime_list[i]+1, step), mean_recalls_list[i], linewidth=2, label=\"k=\"+str(k_list[i]))\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show() \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
